{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# From surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x10d5492b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import Dataset\n",
    "\n",
    "ratings = Dataset.load_builtin('ml-100k')\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x10d549c40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.dataset import DatasetAutoFolds\n",
    "\n",
    "def load_ratings_from_surprise() -> DatasetAutoFolds:\n",
    "    ratings = Dataset.load_builtin('ml-100k')\n",
    "    return ratings\n",
    "\n",
    "load_ratings_from_surprise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x103e5f4c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import Reader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "reader = Reader(line_format = 'user item rating timestamp', sep=',', skip_lines=1)\n",
    "rating_data = Dataset.load_from_file('ratings.csv', reader)\n",
    "rating_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x10d3bea30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(from_surprise : bool = True) -> DatasetAutoFolds:\n",
    "    data = load_ratings_from_surprise() if from_surprise else load_ratings_from_file()\n",
    "    return data\n",
    "\n",
    "data = get_data(from_surprise=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Split data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.trainset.Trainset at 0x103ed56d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1651)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.n_users, train.n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "model = SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x103ed5250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x10d4776d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.trainset import Trainset\n",
    "from  surprise.prediction_algorithms.algo_base import AlgoBase\n",
    "\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "\n",
    "def get_trained_model(model_class: AlgoBase, model_kwargs: dict, train_set: Trainset) -> AlgoBase:\n",
    "    model = model_class(sim_options = model_kwargs)\n",
    "    model.fit(train_set)\n",
    "    return model\n",
    "\n",
    "model_kwargs = {'sim_options': {'user_based': False, 'name': 'pearson'}}\n",
    "get_trained_model(KNNBasic, {'user_based': False, 'name': 'pearson'}, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid='907', iid='143', r_ui=5.0, est=4.972875011006883, details={'was_impossible': False}),\n",
       " Prediction(uid='371', iid='210', r_ui=4.0, est=4.311637716684491, details={'was_impossible': False}),\n",
       " Prediction(uid='218', iid='42', r_ui=4.0, est=3.2653922837229876, details={'was_impossible': False}),\n",
       " Prediction(uid='829', iid='170', r_ui=4.0, est=3.945460496452226, details={'was_impossible': False}),\n",
       " Prediction(uid='733', iid='277', r_ui=1.0, est=3.0020211542011297, details={'was_impossible': False}),\n",
       " Prediction(uid='363', iid='1512', r_ui=1.0, est=3.0604995636621077, details={'was_impossible': False}),\n",
       " Prediction(uid='193', iid='487', r_ui=5.0, est=3.7043226721326517, details={'was_impossible': False}),\n",
       " Prediction(uid='808', iid='313', r_ui=5.0, est=4.751743865630858, details={'was_impossible': False}),\n",
       " Prediction(uid='557', iid='682', r_ui=2.0, est=3.060165137476779, details={'was_impossible': False}),\n",
       " Prediction(uid='774', iid='196', r_ui=3.0, est=2.6485242952388544, details={'was_impossible': False})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.test(test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9356351043753047"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.rmse(predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.7381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7380753896138278"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mae(predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "def evaluate_model(model: AlgoBase, test_set: [(int, int, float)]) -> dict:\n",
    "    predictions = model.test(test_set)\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['RMSE'] = accuracy.rmse(predictions, verbose=False)\n",
    "    metrics_dict['MAE'] = accuracy.rmse(predictions, verbose=False)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular codeÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 0.980150596704479, 'MAE': 0.980150596704479}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "def train_and_evalute_model_pipeline(model_class: AlgoBase, model_kwargs: dict = {},\n",
    "                                     from_surprise: bool = True,\n",
    "                                     test_size: float = 0.2) -> (AlgoBase, dict):\n",
    "    data = get_data(from_surprise)\n",
    "    train_set, test_set = train_test_split(data, test_size, random_state=42)\n",
    "    model = get_trained_model(model_class, model_kwargs, train_set)\n",
    "    metrics_dict = evaluate_model(model, test_set)\n",
    "    return model, metrics_dict\n",
    "\n",
    "my_model, metrics_dict = train_and_evalute_model_pipeline(KNNBasic)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x1170dbd30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trained_model(KNNBasic, {'user_based': False, 'name': 'pearson'}, train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN user based cosine': {'RMSE': 1.0193536815834319,\n",
       "  'MAE': 1.0193536815834319},\n",
       " 'KNN user based pearson': {'RMSE': 1.0150350905205965,\n",
       "  'MAE': 1.0150350905205965},\n",
       " 'KNN item based cosine': {'RMSE': 1.0264295933767333,\n",
       "  'MAE': 1.0264295933767333},\n",
       " 'KNN item based pearson': {'RMSE': 1.041104054968961,\n",
       "  'MAE': 1.041104054968961}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "benchmark_dict = {}\n",
    "\n",
    "model_kwargs = {'user_based': True, 'name': 'cosine'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN user based cosine'] = metrics_dict\n",
    "\n",
    "model_kwargs = {'user_based': True, 'name': 'pearson'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN user based pearson'] = metrics_dict\n",
    "\n",
    "model_kwargs = {'user_based': False, 'name': 'cosine'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN item based cosine'] = metrics_dict\n",
    "\n",
    "model_kwargs = {'user_based': False, 'name': 'pearson'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN item based pearson'] = metrics_dict\n",
    "\n",
    "\n",
    "benchmark_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN user based with cosine similarity': {'RMSE': 1.0193536815834319,\n",
       "  'MAE': 1.0193536815834319},\n",
       " 'KNN user based with pearson similarity': {'RMSE': 1.0150350905205965,\n",
       "  'MAE': 1.0150350905205965}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_dict = {}\n",
    "\n",
    "model_dict_list = [\n",
    "    {\n",
    "        'model_name' : 'KNN user based with cosine similarity',\n",
    "        'model_class' : KNNBasic,\n",
    "        'model_kwargs' : {'user_based': True, 'name': 'cosine'}\n",
    "    },\n",
    "    {\n",
    "        'model_name' : 'KNN user based with pearson similarity',\n",
    "        'model_class' : KNNBasic,\n",
    "        'model_kwargs' : {'user_based': True, 'name': 'pearson'}\n",
    "    },\n",
    "]\n",
    "\n",
    "for model_dict in model_dict_list:\n",
    "    model, metrics_dict = train_and_evalute_model_pipeline(\n",
    "        model_dict['model_class'], model_dict['model_kwargs'])\n",
    "    benchmark_dict[model_dict['model_name']] = metrics_dict\n",
    "    model_dict['fitted_model'] = model\n",
    "    \n",
    "benchmark_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.0167  1.0030  1.0096  1.0155  1.0185  1.0126  0.0057  \n",
      "MAE (testset)     0.8036  0.7969  0.8024  0.8065  0.8088  0.8036  0.0040  \n",
      "Fit time          1.94    1.78    1.87    1.83    1.78    1.84    0.06    \n",
      "Test time         4.64    4.23    4.68    4.37    4.38    4.46    0.17    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.01666025, 1.002967  , 1.00956277, 1.01546667, 1.01852717]),\n",
       " 'test_mae': array([0.8035897 , 0.79691282, 0.80239156, 0.80647929, 0.80876319]),\n",
       " 'fit_time': (1.9350700378417969,\n",
       "  1.7831077575683594,\n",
       "  1.8685150146484375,\n",
       "  1.8335139751434326,\n",
       "  1.7765541076660156),\n",
       " 'test_time': (4.640166997909546,\n",
       "  4.229506015777588,\n",
       "  4.677316904067993,\n",
       "  4.371628999710083,\n",
       "  4.377537965774536)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "cross_validate(model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_rmse': array([1.52341906, 1.52687755, 1.52257202]), 'test_mae': array([1.2254386 , 1.22571624, 1.22455314]), 'fit_time': (0.1397261619567871, 0.16882991790771484, 0.13615703582763672), 'test_time': (0.6412389278411865, 0.4476642608642578, 0.27816009521484375)}\n"
     ]
    }
   ],
   "source": [
    "from surprise import NMF\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "algo = NormalPredictor()\n",
    "perf = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3)\n",
    "print(perf)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_rmse': array([0.9774521, 0.9769264, 0.972586 ]), 'test_mae': array([0.76688009, 0.76871545, 0.76305713]), 'fit_time': (6.107399940490723, 5.762816905975342, 6.271279811859131), 'test_time': (0.3113729953765869, 0.4848639965057373, 0.48643994331359863)}\n"
     ]
    }
   ],
   "source": [
    "from surprise import NMF\n",
    "algo = NMF()\n",
    "perf = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_rmse': array([0.95240962, 0.94403124, 0.94761104]), 'test_mae': array([0.75129077, 0.74587081, 0.74796527]), 'fit_time': (5.432853937149048, 5.1817307472229, 5.17937707901001), 'test_time': (0.31631994247436523, 0.43987035751342773, 0.44788479804992676)}\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "algo = SVD()\n",
    "perf = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_rmse': array([0.9337957 , 0.92472426, 0.92509491]), 'test_mae': array([0.73243991, 0.72842871, 0.72799848]), 'fit_time': (170.6865348815918, 158.2787880897522, 173.03230786323547), 'test_time': (6.3825578689575195, 6.7504048347473145, 6.134533882141113)}\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVDpp\n",
    "algo = SVDpp()\n",
    "perf = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3)\n",
    "print(perf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def get_user_recommendation\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
